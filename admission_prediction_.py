# -*- coding: utf-8 -*-
"""Admission_prediction_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FMh671vEAP9Mu-X8oDPEYYsZheczt0q9
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing essential libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

# Loading the dataset
df = pd.read_csv('/content/drive/MyDrive/admission_prediction/admission_predict.csv')

"""Exploring the dataset"""

df.shape

df.head()

df.tail()

df.columns

df.info()

df.describe().T

df.isnull().any()

df = df.rename(columns={'GRE Score': 'GRE', 'TOEFL Score': 'TOEFL', 'LOR ': 'LOR', 'Chance of Admit ': 'Probability'})
df.head()

"""Data Visualization

"""

# Creating 4x2 grid of subplots
fig, axes = plt.subplots(4, 2, figsize=(12, 20))
axes = axes.flatten()

# Plot histograms for each feature
for i, column in enumerate(df.columns[1:8]):
    axes[i].hist(df[column], rwidth=0.7, color='skyblue', edgecolor='black')
    axes[i].set_title(column)

# Turn off remaining unused subplots
axes[7].axis('off')

plt.tight_layout()
plt.show()

# Visualizing the feature GRE
fig = plt.hist(df['GRE'], rwidth=0.7)
plt.title("Distribution of GRE Scores")
plt.xlabel('GRE Scores')
plt.ylabel('Count')
plt.show()

# Visualizing the feature TOEFL
fig = plt.hist(df['TOEFL'], rwidth=0.7)
plt.title('Distribution of TOEFL Scores')
plt.xlabel('TOEFL Scores')
plt.ylabel('Count')
plt.show()

# Visualizing the feature University Rating
fig = plt.hist(df['University Rating'], rwidth=0.7)
plt.title('Distribution of University Rating')
plt.xlabel('University Rating')
plt.ylabel('Count')
plt.show()

# Visualizing the feature SOP
fig = plt.hist(df['SOP'], rwidth=0.7)
plt.title('Distribution of SOP')
plt.xlabel('SOP Rating')
plt.ylabel('Count')
plt.show()

# Visualizing the feature LOR
fig = plt.hist(df['LOR'], rwidth=0.7)
plt.title('Distribution of LOR Rating')
plt.xlabel('LOR Rating')
plt.ylabel('Count')
plt.show()

# Visualizing the feature CGPA
fig = plt.hist(df['CGPA'], rwidth=0.7)
plt.title('Distribution of CGPA')
plt.xlabel('CGPA')
plt.ylabel('Count')
plt.show()

# Visualizing the feature Research Papers
fig = plt.hist(df['Research'], rwidth=0.7)
plt.title('Distribution of Research Papers')
plt.xlabel('Research')
plt.ylabel('Count')
plt.show()

"""Data Cleaning"""

# Removing the serial no. column
df.drop('Serial No.', axis='columns', inplace=True)
df.head()

"""
Replacing the 0 values from ['GRE','TOEFL','University Rating','SOP','LOR','CGPA'] by NaN
because 0 is not a valid score
"""
df_copy = df.copy(deep=True)
df_copy[['GRE','TOEFL','University Rating','SOP','LOR','CGPA']] = df_copy[['GRE','TOEFL','University Rating','SOP','LOR','CGPA']].replace(0, np.nan)
df_copy.isnull().sum()

"""Model Building"""

# Splitting the dataset in features and label
X = df_copy.drop('Probability', axis='columns')
y = df_copy['Probability']

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor

# Creating a function to calculate best model for this problem
def find_best_model(X, y):
    models = {
        'linear_regression': {
            'model': LinearRegression(),
            'parameters': {
                'positive': [True,False]
            }
        },

        'lasso': {
            'model': Lasso(),
            'parameters': {
                'alpha': [1,2],
                'selection': ['random', 'cyclic']
            }
        },

        'svr': {
            'model': SVR(),
            'parameters': {
                'gamma': ['auto','scale']
            }
        },


        'knn': {
            'model': KNeighborsRegressor(algorithm='auto'),
            'parameters': {
                'n_neighbors': [2,5,10,20]
            }
        }
    }

    scores = []
    for model_name, model_params in models.items():
        gs = GridSearchCV(model_params['model'], model_params['parameters'], cv=5, return_train_score=False)
        gs.fit(X, y)
        scores.append({
            'model': model_name,
            'best_parameters': gs.best_params_,
            'score': gs.best_score_
        })

    return pd.DataFrame(scores, columns=['model','best_parameters','score'])

find_best_model(X, y)

"""Since the Linear Regression algorithm has the highest accuracy, the model selected for this problem is Linear Regression."""

# Using cross_val_score for gaining highest accuracy
from sklearn.model_selection import cross_val_score
scores = cross_val_score(LinearRegression(positive=True), X, y, cv=5)
print('Highest Accuracy : {}%'.format(round(sum(scores)*100/len(scores)), 3))

# Splitting the dataset into train and test samples
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)
print(len(X_train), len(X_test))

# Creating Linear Regression Model
model = LinearRegression(positive=True)
model.fit(X_train, y_train)
model.score(X_test, y_test)

"""Predicting the values using our trained model"""

# Prediction 1
# Input in the form : GRE, TOEFL, University Rating, SOP, LOR, CGPA, Research
print('Chance of getting into UCLA is {}%'.format(round(model.predict([[337, 118, 4, 4.5, 4.5, 9.65, 0]])[0]*100, 3)))

# Prediction 2
# Input in the form : GRE, TOEFL, University Rating, SOP, LOR, CGPA, Research
print('Chance of getting into UCLA is {}%'.format(round(model.predict([[320, 113, 2, 2.0, 2.5, 8.64, 1]])[0]*100, 3)))

# ==========================================================
# Graduate Admission Prediction - Enhanced Linear Regression
# ==========================================================

# 1. Import Required Libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 2. Feature Engineering
# We add a few meaningful interaction and squared features to help the linear model capture more complex relationships,
# but we avoid full polynomial expansion to keep the model simple, interpretable, and less prone to overfitting.
X_enhanced = X.copy()
X_enhanced['GRE_TOEFL'] = X_enhanced['GRE'] * X_enhanced['TOEFL'] / 1000  # Interaction, scaled
X_enhanced['GRE_CGPA'] = X_enhanced['GRE'] * X_enhanced['CGPA'] / 100     # Interaction, scaled
X_enhanced['GRE_sq'] = X_enhanced['GRE'] ** 2 / 1000                      # Squared, scaled
X_enhanced['CGPA_sq'] = X_enhanced['CGPA'] ** 2                           # Squared
X_enhanced['Combined_Rating'] = (X_enhanced['University Rating'] + X_enhanced['SOP'] + X_enhanced['LOR']) / 3  # Mean rating

print(f"Enhanced features shape: {X_enhanced.shape}")
print("New features: GRE_TOEFL, GRE_CGPA, GRE_sq, CGPA_sq, Combined_Rating")

# 3. Split Data into Train and Test Sets
X_train_enhanced, X_test_enhanced, y_train, y_test = train_test_split(
    X_enhanced, y, test_size=0.20, random_state=5
)

# 4. Train the Enhanced Linear Regression Model
enhanced_lr = LinearRegression(positive=True)
enhanced_lr.fit(X_train_enhanced, y_train)

# 5. Evaluate Model Performance
y_pred = enhanced_lr.predict(X_test_enhanced)
r2_score = enhanced_lr.score(X_test_enhanced, y_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
accuracy = 100 * (1 - rmse / (y.max() - y.min()))  # Accuracy as 1 - normalized RMSE

print(f"Enhanced Linear Regression R² score: {r2_score:.4f}")
print(f"Test RMSE: {rmse:.4f}")
print(f"Test Accuracy: {accuracy:.2f}%")

# 6. Visualization Functions
def visualize_model_performance():
    # Bar plot for accuracy
    plt.figure(figsize=(5, 7))
    plt.bar(['Enhanced Linear Regression'], [accuracy], color='#3498db')
    plt.text(0, accuracy + 0.5, f'{accuracy:.2f}%', ha='center', va='bottom')
    plt.ylim(70, 100)
    plt.title('Model Performance', fontsize=14)
    plt.ylabel('Accuracy (%)')
    plt.xlabel('Model')
    plt.legend()
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.show()

    # Feature importance (absolute value of coefficients)
    plt.figure(figsize=(10, 4))
    feature_names = list(X_enhanced.columns)
    coefficients = enhanced_lr.coef_
    importance = np.abs(coefficients)
    indices = np.argsort(importance)[::-1]
    plt.title('Feature Importance (Linear Model Coefficients)', fontsize=14)
    plt.bar(range(len(feature_names)), importance[indices], color='#2ecc71', align='center')
    plt.xticks(range(len(feature_names)), [feature_names[i] for i in indices], rotation=45, ha='right')
    plt.tight_layout()
    plt.grid(axis='y', alpha=0.3)
    plt.show()

    # Actual vs Predicted scatter plot
    plt.figure(figsize=(7, 4))
    plt.scatter(y_test, y_pred, alpha=0.7, color='#3498db')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    plt.xlabel('Actual Probability')
    plt.ylabel('Predicted Probability')
    plt.title('Actual vs Predicted Admission Probability', fontsize=14)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

visualize_model_performance()

# 7. Prediction Function for New Data
def predict_admission_chance(gre, toefl, univ_rating, sop, lor, cgpa, research):
    """
    Predicts the chance of admission using the enhanced linear regression model.
    Input: All features as numbers (same order as training)
    Output: Probability in percent (rounded to 2 decimals)
    """
    input_data = pd.DataFrame({
        'GRE': [gre],
        'TOEFL': [toefl],
        'University Rating': [univ_rating],
        'SOP': [sop],
        'LOR': [lor],
        'CGPA': [cgpa],
        'Research': [research]
    })
    # Add engineered features
    input_enhanced = input_data.copy()
    input_enhanced['GRE_TOEFL'] = input_enhanced['GRE'] * input_enhanced['TOEFL'] / 1000
    input_enhanced['GRE_CGPA'] = input_enhanced['GRE'] * input_enhanced['CGPA'] / 100
    input_enhanced['GRE_sq'] = input_enhanced['GRE'] ** 2 / 1000
    input_enhanced['CGPA_sq'] = input_enhanced['CGPA'] ** 2
    input_enhanced['Combined_Rating'] = (input_enhanced['University Rating'] + input_enhanced['SOP'] + input_enhanced['LOR']) / 3
    prediction = enhanced_lr.predict(input_enhanced)[0] * 100
    return round(prediction, 2)

# 8. Example Predictions
print("\n" + "="*60)
print("Example Predictions")
print("="*60)
print(f"Chance of getting into UCLA (Profile 1): {predict_admission_chance(337, 118, 4, 4.5, 4.5, 9.65, 0)}%")
print(f"Chance of getting into UCLA (Profile 2): {predict_admission_chance(320, 113, 2, 2.0, 2.5, 8.64, 1)}%")

# 9. Final Model Summary
print("\n" + "="*60)
print("FINAL MODEL SUMMARY")
print("="*60)
print(f"Enhanced Linear Regression R² Score: {r2_score:.4f}")
print(f"Test Accuracy: {accuracy:.2f}%")
print("="*60)

import joblib

# Save the trained model
joblib.dump(model, 'admission_model.pkl')

from google.colab import files
files.download('admission_model.pkl')

!pip install pyngrok flask flask-cors joblib --quiet

from pyngrok import ngrok

ngrok.set_auth_token("2wkR1ZhCdtUCfrMGNnH6KEL5YYS_6DpjsAHZU6izVWtJdt65t")
from flask import Flask, request, jsonify
from flask_cors import CORS
import joblib

# Load your model
model = joblib.load('admission_model.pkl')

app = Flask(__name__)
CORS(app)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    features = [
        data['gre'],
        data['toefl'],
        data['university_rating'],
        data['sop'],
        data['lor'],
        data['cgpa'],
        data['research']
    ]
    prediction = model.predict([features])[0]
    return jsonify({'probability': float(prediction)})

# Open a ngrok tunnel to the Flask app
public_url = ngrok.connect(5000)
print(' * ngrok tunnel:', public_url)

app.run()